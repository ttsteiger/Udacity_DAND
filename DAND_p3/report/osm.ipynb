{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle OpenStreetMap Data\n",
    "\n",
    "In this project I will have a look at the OpenStreeMap data of the area around Zürich in Switzerland. The goal is to apply data wrangling techniques to asses the data quality and clean possible uniformities and inconsistencies. Finally, the data will be passed to a structured SQL database using the SQLite engine. Basic SQL querries and aggregations as well as the `sqlite3` Python API will be used to gather statistics and interesting insights about the data set.\n",
    "\n",
    "During the project the following Python libraries were used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import mplleaflet\n",
    "import pprint\n",
    "import re\n",
    "import sqlite3\n",
    "import xml.etree.cElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `csv` and `xml` libraries were used for data gathering and parsing while `sqlite3` was required for database interactions. `mplleaflet` is a simple library that allows to convert `matplotlib` visualisations into zoomable Leaflet maps.\n",
    "\n",
    "## Map Area\n",
    "\n",
    "Zürich, Switzerland\n",
    "\n",
    "[https://www.openstreetmap.org/relation/1682248](https://www.openstreetmap.org/relation/1682248)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "I created a few simple helper functions to explore the xml data. They can be found in the [`explore.py`](https://github.com/ttsteiger/Udacity_DAND/blob/master/DAND_p3/explore.py) file. They were used to  count the different root elements and their children to get a feeling for the structure and dimensions of the file.\n",
    "\n",
    "A output returned by the functions can be found in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_file_structure' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5270fd9d7e45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'zurich.osm'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mroot_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_file_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Root elements and their attributes:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_file_structure' is not defined"
     ]
    }
   ],
   "source": [
    "# osm xml file\n",
    "file = 'zurich.osm'\n",
    "\n",
    "root_attributes, child_attributes = get_file_structure(file)\n",
    "\n",
    "print(\"Root elements and their attributes:\")\n",
    "pprint.pprint(root_attributes)\n",
    "print()\n",
    "\n",
    "print(\"Children and their attributes within the root elements:\")\n",
    "pprint.pprint(child_attributes)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results I could get a clear picture of the tree structure of the OSM XML file.\n",
    "\n",
    "```xml\n",
    "<node id lat lon user uid version changeset timestamp> [2 018 926]\n",
    "    <tag k v /> [340 957]\n",
    "</node>\n",
    "<way id user uid version changeset timestamp> [302 080]\n",
    "    <tag k v /> [936 274]\n",
    "    <nd ref /> [2 530 372]\n",
    "</way>\n",
    "<relation id user uid version changeset timestamp> [4 765]\n",
    "    <tag k v /> [21 916]\n",
    "    <member ref role type /> [135 999]\n",
    "</relation>\n",
    "```\n",
    "\n",
    "We can see that the data is split into three different root element categories: node, way, and relation. \n",
    "\n",
    "Nodes consist of single points in space defined by their latitude and longitude. Ways contain tags and ordered lists of nodes that define their shape. This nodes are clearly identified by the unique node id in the `ref` attribute of their `nd` children. Relations are used to describe logical or geographical relationships between their member childrens than can either be `node` or `way` types. All three different root elements can have `tag` children that contain further information stored as key and value pairs. The numbers inbetween square brackets denote the number of occurences of the element in that line.\n",
    "\n",
    "My next step in the data exploration process was to have a look at the different keys and values that appear in the `tag` children of the different root elements. Therefore I used further exploration helper functions to count the different keys in the tag children of the specified root element and to count the different values associated with a certain key.\n",
    "\n",
    "To show how these functions work, they will be used in the code block below to print out the most frequent keys within all tag elements and the different surface type values that occur in way root elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different keys within all root elements\n",
    "tag_keys = get_tag_keys(file, ['node', 'way', 'relation'])\n",
    "\n",
    "# most frequent values associated with \"building\" key\n",
    "way_tag_surface_values = get_tag_key_values(file, ['way'], 'surface')\n",
    "\n",
    "# print out top 20 entries of both lists\n",
    "print()\n",
    "pprint.pprint(tag_keys[:20])\n",
    "print()\n",
    "pprint.pprint(way_tag_surface_values[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Auditing - Problems Encountered in the Map\n",
    "\n",
    "While browsing trough the values of some tag key attributes I did not discover any major data inconsistencies but only a few formatting variations. I noticed four main formatting problems:\n",
    "\n",
    "* Housenumbers in Switzerland are sometimes combinations of numbers and letters. Flats in the same apartement building for example might have the same housenumber but are then distinguished by adding a letter after the number (1A, 1B, 1C,...). I want all of these letters to be uppercase.\n",
    "\n",
    "* If cities or villages with the same name appear in multiple locations, they are usually suffixed with abbreviation of the state they are in. For the canton of Zürich, this abbreviation is ZH. I want all locations that have this suffix to be in the same format, \"(ZH)\". Furthermore, I want to fix a few abbreviations that occur, such as b., a., a.A..\n",
    "\n",
    "* Not all website values have the same formatting. I will harmonize them so that each entry starts with \"http://www.\".\n",
    "\n",
    "* The format for phone numbers should be standardized to \"+41 xx xxx xx xx\".\n",
    "\n",
    "The `audit()` function from the [`audit.py`](https://github.com/ttsteiger/Udacity_DAND/blob/master/DAND_p3/audit.py) file was used to print out data entries that do not follow the desired specifications. This allowed to see which patterns needed to be cleaned. As an example the audit function for phone numbers is shown below. If the `audit_phone_number()` function finds a number that does not follow the standard Swiss format it will be added to the `bad_phone_number` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audit phone\n",
    "def audit_phone_number(phone_number, bad_phone_numbers):\n",
    "    # check if phone number follows the standard format\n",
    "    phone_number_re = re.compile('\\+[0-9]{2} [0-9]{2} [0-9]{3} [0-9]{2} [0-9]{2}') # +41 xx xxx xx xx\n",
    "     \n",
    "    m = phone_number_re.search(phone_number)\n",
    "    \n",
    "    if not m:\n",
    "        bad_phone_numbers.append(phone_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address all the issues shown above I created cleaning functions that will be applied to the respective tag values before storing them in the csv files that will be imported into the database. To continue the example form above only the cleaning function for phone numbers is shown here. All the others can be found in the [`clean.py`](https://github.com/ttsteiger/Udacity_DAND/blob/master/DAND_p3/clean.py) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_phone_number(phone_number):\n",
    "    phone_number_re = re.compile('\\+[0-9]{2} [0-9]{2} [0-9]{3} [0-9]{2} [0-9]{2}')\n",
    "    m = phone_number_re.search(phone_number)\n",
    "    \n",
    "    if not m:\n",
    "        # remove '(0)', '-' and ' '\n",
    "        for ch in [\"-\", \"(0)\", \" \"]:\n",
    "            if ch in phone_number:\n",
    "                phone_number = phone_number.replace(ch, \"\")\n",
    "        \n",
    "        # set correct starting sequence \n",
    "        if phone_number[:3] != \"+41\":\n",
    "            if phone_number[:2] == \"04\" or phone_number[:2] == \"07\":\n",
    "                phone_number = \"+41{}\".format(phone_number[1:])\n",
    "        \n",
    "        # set spacing\n",
    "        phone_number = \"{} {} {} {} {}\".format(phone_number[:3], phone_number[3:5], phone_number[5:8], \n",
    "                                               phone_number[8:10], phone_number[10:])\n",
    "\n",
    "    return phone_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Database\n",
    "\n",
    "To be able to store the OSM data in a SQL database, the xml input file will be transformed into a tabular dictionary format. This format can easily be stored in csv files which will then be imported into the different SQL tables.\n",
    "\n",
    "The `shape_element()` function takes an iterparse element object as input and returns a formatted dictionary. If the element top level tag is equal to \"node\", the function will return the following dictionary structure:\n",
    "```\n",
    "{'node': {'node_id': ...,\n",
    "          'lat': ...,\n",
    "          'lon': ...,\n",
    "          'user': ...,\n",
    "          'uid': ...,\n",
    "          'version': ...,\n",
    "          'changeset': ...,\n",
    "          'timestamp': ...},\n",
    " 'node_tags': [{'node_id': ...,\n",
    "               'key': ...,\n",
    "               'value': ...,\n",
    "               'type': ...},\n",
    "               {'node_id': ...,\n",
    "               'key': ...,\n",
    "               'value': ...,\n",
    "               'type': ...},\n",
    "               ...]\n",
    "}\n",
    "```\n",
    "The `node` field contains all the attributes of the node root element. The `node_tags` field contains a list with dictionaries for all the secondary tag children of the passed root element. 'node_tags' entries are are connected to their parent elements based on the unique `node_id` values. Furthermore, if the tag `k` attribute contains any problematic characters, the tag is ignored, but if the tag contains a \":\", the tag `type` well be set to the characters coming before the \":\". If the tag key does not contain a colon, the `type` field will be set to \"regular\". Before any `v` attributes are written to the `value` field, they are cleaned using the cleaning functions from the section above.\n",
    "\n",
    "Similar dictionaries were shaped for \"way\" and \"relation\" root elements. The shaping function can be found in the [`shape.py`](https://github.com/ttsteiger/Udacity_DAND/blob/master/DAND_p3/shape.py) file. The main purpose of the shaping functions is to convert the XML elements into a format that can easily be stored in a csv file. Each csv file will later represent a distinct table in the SQL database. The `convert_to_csv()` function in the [`main.py`](https://github.com/ttsteiger/Udacity_DAND/blob/master/DAND_p3/main.py) file iterates trough the all root elements, collects the data for each one in a shaped dictionary and writes them line-by-line to the different output files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can import the csv files and store the data in a databse we need to create the empty SQL databse and define the structure of its tables. Instead of doing this manually trough the SQLite command line interface, I used the Python API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(conn, create_table_sql):\n",
    "    \"\"\"Create new sql table based on the command given as string.\"\"\"\n",
    "    c = conn.cursor()\n",
    "    c.execute(create_table_sql)\n",
    "\n",
    "# SQL commands\n",
    "create_nodes_table_sql = \"\"\"CREATE TABLE IF NOT EXISTS nodes (\n",
    "                                node_id INTEGER PRIMARY KEY NOT NULL,\n",
    "                                lat REAL,\n",
    "                                lon REAL,\n",
    "                                user TEXT,\n",
    "                                uid INTEGER,\n",
    "                                version INTEGER,\n",
    "                                changeset INTEGER,\n",
    "                                timestamp TEXT\n",
    "                            );\"\"\"\n",
    "\n",
    "create_nodes_tags_table_sql = \"\"\"CREATE TABLE IF NOT EXISTS nodes_tags (\n",
    "                                     node_id INTEGER,\n",
    "                                     key TEXT,\n",
    "                                     value TEXT,\n",
    "                                     type TEXT,\n",
    "                                     FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    "                                 );\"\"\"\n",
    "\n",
    "create_ways_table_sql = \"\"\"CREATE TABLE IF NOT EXISTS ways (\n",
    "                               way_id INTEGER PRIMARY KEY NOT NULL,\n",
    "                               user TEXT,\n",
    "                               uid INTEGER,\n",
    "                               version INTEGER,\n",
    "                               changeset INTEGER,\n",
    "                               timestamp TEXT\n",
    "                           );\"\"\"\n",
    "\n",
    "create_ways_tags_table_sql = \"\"\"CREATE TABLE IF NOT EXISTS ways_tags (\n",
    "                                    way_id INTEGER,\n",
    "                                    key TEXT,\n",
    "                                    value TEXT,\n",
    "                                    type TEXT,\n",
    "                                    FOREIGN KEY (way_id) REFERENCES ways(id)\n",
    "                                );\"\"\"\n",
    "\n",
    "create_ways_nodes_table_sql = \"\"\"CREATE TABLE IF NOT EXISTS ways_nodes (\n",
    "                                     way_id INTEGER,\n",
    "                                     node_id INTEGER,\n",
    "                                     position INTEGER,\n",
    "                                     FOREIGN KEY (way_id) REFERENCES ways(id),\n",
    "                                     FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    "                                 );\"\"\"\n",
    "create_relations_table_sql = \"\"\"CREATE TABLE IF NOT EXISTS relations (\n",
    "                                    relation_id INTEGER PRIMARY KEY NOT NULL,\n",
    "                                    user TEXT,\n",
    "                                    uid INTEGER,\n",
    "                                    version INTEGER,\n",
    "                                    changeset INTEGER,\n",
    "                                    timestamp TEXT\n",
    "                                );\"\"\"\n",
    "\n",
    "create_relations_tags_table_sql = \"\"\"CREATE TABLE IF NOT EXISTS relations_tags (\n",
    "                                         relation_id INTEGER,\n",
    "                                         key TEXT,\n",
    "                                         value TEXT,\n",
    "                                         type TEXT,\n",
    "                                         FOREIGN KEY (relation_id) REFERENCES relations(id)\n",
    "                                     );\"\"\"\n",
    "\n",
    "create_relations_members_table_sql = \"\"\"CREATE TABLE IF NOT EXISTS relations_members (\n",
    "                                            relation_id INTEGER,\n",
    "                                            type TEXT,\n",
    "                                            node_id INTEGER,\n",
    "                                            way_id INTEGER,\n",
    "                                            role TEXT,\n",
    "                                            FOREIGN KEY (relation_id) REFERENCES relations(id),\n",
    "                                            FOREIGN KEY (node_id) REFERENCES nodes(id),\n",
    "                                            FOREIGN KEY (way_id) REFERENCES ways(id)\n",
    "                                        );\"\"\"\n",
    "\n",
    "# list containing all querries to create the tables\n",
    "sql_tables = [create_nodes_table_sql, create_nodes_tags_table_sql,\n",
    "              create_ways_table_sql, create_ways_tags_table_sql, create_ways_nodes_table_sql,\n",
    "              create_relations_table_sql, create_relations_tags_table_sql, create_relations_members_table_sql]\n",
    "\n",
    "# database name\n",
    "db = \"zurich.db\"\n",
    "\n",
    "# create database and set up the tables\n",
    "conn = sqlite3.connect(db) # creates new db if it does not already exist\n",
    "for t in sql_tables:\n",
    "    create_table(conn, t)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the csv files can be imported into the SQL database. For this step I also used the SQLite Python API. The fields of each line of the csv file are aggregated in a list of lists. The `executemany()` method is then used to execute the SQL command against all the given parameter sequences.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_csv_file_into_db(db_name, file_name, table_name):\n",
    "    \"\"\"\n",
    "    Import csv file into database. The table needs to exist already.\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        f_reader = csv.DictReader(f)\n",
    "        header = f_reader.fieldnames\n",
    "        \n",
    "        # list of lists containing the csv rows for the db import\n",
    "        db_data = [[r[k] for k in header] for r in f_reader]\n",
    "    \n",
    "    # construct sql command\n",
    "    columns = \"({})\".format(', '.join(header)) # concatenate field names\n",
    "    values = \"({})\".format(', '.join(['?' for i in range(len(header))])) # number of ? equal to number of fields\n",
    "    sql_command = \"INSERT INTO {} {} VALUES {};\".format(table_name, columns, values)\n",
    "    \n",
    "    # connect to db and import the data\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.executemany(sql_command, db_data)\n",
    "    conn.commit()\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Import of {} successful!\".format(file_name))\n",
    "\n",
    "# list containing all csv file paths\n",
    "files = [nodes_path, nodes_tags_path, \n",
    "         ways_path, ways_tags_path, ways_nodes_path,\n",
    "         relations_path, relations_tags_path, relations_members_path]\n",
    "\n",
    "# get table names for the import function\n",
    "conn = sqlite3.connect(db)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = [t[0] for t in cursor.fetchall()]\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# write each csv file to its correpsonding table\n",
    "for f, t in zip(files, tables):\n",
    "    import_csv_file_into_db(db, f, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Data\n",
    "\n",
    "This section contains basic information and statistics about the SQL database.\n",
    "\n",
    "During data wrangling and the set-up of the SQL database the following files were created and used:\n",
    "```sh\n",
    "zurich.osm ................. 471.1 MB\n",
    "nodes.csv .................. 160.1 MB\n",
    "nodes_tags.csv ............. 12.2 MB\n",
    "ways.csv ................... 17.4 MB\n",
    "ways_nodes.csv ............. 59.9 MB\n",
    "ways_tags.csv .............. 32.2 MB\n",
    "relations.csv .............. 0.3 MB\n",
    "relations_tags.csv ......... 0.8 MB\n",
    "relations_members.csv ...... 3.6 MB\n",
    "zurich.db .................. 261.3 MB\n",
    "```\n",
    "\n",
    "The following figure shows the structure of the \"zurich.db\" database. It displays all the tables with their respective columns and how they are interconnected.\n",
    "\n",
    "![Figure 1: DB Schema](dand_p3_db_schema.png \"DB Schema\")\n",
    "\n",
    "Explain relations_members table with way_id and node_id!!!\n",
    "\n",
    "### Number of nodes, ways and relations\n",
    "\n",
    "Using SQL commands with the structure displayed below I counted the number of entries in each table containing the three different types of root elements.\n",
    "\n",
    "```SQL\n",
    "SELECT count(*) FROM nodes;\n",
    "```\n",
    "\n",
    "The results of the querries are summarized in the following table:\n",
    "\n",
    "| Table             | Number of Rows |\n",
    "| :---------------- | -------------: |\n",
    "| nodes             | 2'018'926      |\n",
    "| nodes_tags        | 340'957        |\n",
    "| ways              | 302'080        |\n",
    "| ways_tags         | 936'274        |\n",
    "| ways_nodes        | 2'530'372      |\n",
    "| relations         | 4'765          |\n",
    "| relations_tags    | 21'916         |\n",
    "| relations_members | 135'999        |\n",
    "\n",
    "These numbers correspond with the number of elements we counted in the XML file during data exploration.\n",
    "\n",
    "### User Statistics\n",
    "\n",
    "Number of unique users that contributed to the dataset:\n",
    "```SQL\n",
    "SELECT count(DISTINCT user) \n",
    "FROM (SELECT user FROM nodes \n",
    "      UNION ALL SELECT user FROM ways \n",
    "      UNION ALL SELECT user FROM relations);\n",
    "```\n",
    "```sh\n",
    "2255\n",
    "```\n",
    "\n",
    "Top 10 users regarding number of contributions:\n",
    "```SQL\n",
    "SELECT user, num, num / 2325771.0 * 100.0 AS perc\n",
    "FROM(\n",
    "    SELECT user, \n",
    "           count(*) AS num\n",
    "    FROM (SELECT user FROM nodes \n",
    "          UNION ALL SELECT user FROM ways \n",
    "          UNION ALL SELECT user FROM relations)\n",
    "    GROUP BY user\n",
    "    ORDER BY num DESC)\n",
    "LIMIT 10;\n",
    "```\n",
    "```sh\n",
    "user         | num    | perc\n",
    "-------------|--------|-----------------\n",
    "mdk          | 469461 | 20.1851773024945\n",
    "SimonPoole   | 270031 | 11.6103864051964\n",
    "Sarob        | 145852 | 6.27112471520197\n",
    "feuerstein   | 107720 | 4.63158238708798\n",
    "joshx        | 76684  | 3.29714318391622\n",
    "ponte1112    | 76593  | 3.2932305029171\n",
    "ueliw0       | 62249  | 2.67648878586929\n",
    "captain_slow | 55334  | 2.37916802643081\n",
    "dulix9       | 47797  | 2.05510344741593\n",
    "kuede        | 39949  | 1.71766695861286\n",
    "```\n",
    "By dividing trough the total number of root elements of 2'325'771 we get the overall contributions of the users as a percentage value. It shows that the top 5 contributiors added about 50 % of all the data entries.\n",
    "\n",
    "## Additional Data Analysis\n",
    "\n",
    "### Amenities\n",
    "Most common amenities:\n",
    "\n",
    "```SQL\n",
    "SELECT value, count(*) AS num\n",
    "FROM (SELECT value FROM nodes_tags WHERE key=\"amenity\"\n",
    "      UNION ALL SELECT value FROM ways_tags WHERE key=\"amenity\"\n",
    "      UNION ALL SELECT value FROM relations_tags WHERE key=\"amenity\")\n",
    "GROUP BY value\n",
    "ORDER BY num DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "```sh\n",
    "value           | num\n",
    "--------------- | ----\n",
    "parking         | 3630\n",
    "bench           | 2803\n",
    "restaurant      | 1767\n",
    "drinking_water  | 1220\n",
    "school          | 827\n",
    "waste_basket    | 817\n",
    "post_box        | 781\n",
    "vending_machine | 642\n",
    "bicycle_parking | 554\n",
    "recycling       | 438\n",
    "```\n",
    "\n",
    "### Restaurants and their Cuisines\n",
    "\n",
    "To have a closer look at the restaurant nodes I extracted their longitudes and latitudes and displayed them in a [Leaflet](http://leafletjs.com/) map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query\n",
    "sql_query = \"\"\"\n",
    "SELECT lon, lat\n",
    "FROM nodes, nodes_tags\n",
    "WHERE nodes_tags.node_id = nodes.node_id\n",
    "AND nodes_tags.key=\"amenity\"\n",
    "AND nodes_tags.value=\"restaurant\";\n",
    "\"\"\"\n",
    "\n",
    "conn = sqlite3.connect('zurich.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(sql_query)\n",
    "results = cursor.fetchall()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# extract lon and lat\n",
    "lon, lat = [x for x, y in results], [y for x, y in results]\n",
    "\n",
    "# plot restaurant positions on mplleaflet map\n",
    "f = plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.scatter(lon, lat)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "mplleaflet.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plot of the 10 most popular restaurant cuisines in Zürich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query\n",
    "sql_query = \"\"\"\n",
    "SELECT nodes_tags.value, count(*) AS num\n",
    "FROM nodes, nodes_tags\n",
    "WHERE nodes_tags.node_id = nodes.node_id\n",
    "AND nodes_tags.key=\"cuisine\"\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY num DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "conn = sqlite3.connect('zurich.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(sql_query)\n",
    "results = cursor.fetchall()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# index, labels and counts for the bar plot\n",
    "ind = range(len(results))\n",
    "l, c = [l for l, c in results], [c for l, c in results]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.bar(ind, c)\n",
    "ax.set(title=\"Most popular Cuisines in Zürich\", ylabel=\"Number of Restaurants\", xticks=ind)\n",
    "ax.set_xticklabels(l, rotation=45)\n",
    "ax.grid(alpha=0.4, axis='y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sports\n",
    "\n",
    "Most popular sports found in Zürich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query\n",
    "sql_query = \"\"\"\n",
    "SELECT value, count(*) AS num\n",
    "FROM (SELECT value FROM nodes_tags WHERE key=\"sport\"\n",
    "      UNION ALL SELECT value FROM ways_tags WHERE key=\"sport\"\n",
    "      UNION ALL SELECT value FROM relations_tags WHERE key=\"sport\")\n",
    "GROUP BY value\n",
    "ORDER BY num DESC\n",
    "LIMIT 20;\n",
    "\"\"\"\n",
    "\n",
    "conn = sqlite3.connect('zurich.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(sql_query)\n",
    "results = cursor.fetchall()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# index, labels and counts for the horizontal bar plot\n",
    "ind = range(len(results))\n",
    "l, c = [l for l, c in results[::-1]], [c for l, c in results[::-1]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.barh(ind, c)\n",
    "ax.set(title=\"Most popular Sports in Zürich\", xlabel=\"Number of Tags\", yticks=ind)\n",
    "ax.set_yticklabels(l, rotation=0)\n",
    "ax.grid(alpha=0.4, axis='x')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Ideas for improving the Data Set\n",
    "\n",
    "The first idea that came to my mind when thinking about OpenStreetMap data is why not use the same information from Google Maps. As Google is a huge corporation with limitless resources their data must be more accurate than that of an open source community. But as Google is return driven, I think they will mainly focus on urban areas and big cities where exact and up-to-data mapping is beneficial for their marketing purposes while the OSM community also focuses on more rural areas for more creative purposes such as hiking, cycling or other types of sports (see most popular sports found in Zurich above). With a simple querry we can also count the number of hiking trails in the `nodes_tags` table which account for about 500 entries.\n",
    "\n",
    "```SQL\n",
    "SELECT count(*) FROM nodes_tags WHERE key='hiking';\n",
    "```\n",
    "```sh\n",
    "498\n",
    "```\n",
    "\n",
    "Therefore I would suggest that one could use the Google Maps API to improve data quality in cities. Map information about shops, restaurants and traffic is probably more accurate there. A big problem here might be that the Google Maps data is owned by Google and we run into copyright issues. But on the other hand, to improve the data in the countryside one could try to combine the map updates with certain sports activities such as Geocaching. The use of mobile GPS during this type of treasure hunt would guarantee accurate location data in remote areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The review of the OpenStreetMap data of the area around Zürich showed me that the data quality is already pretty high. I did not encounter any major problems regarding data validity and accuracy but just a few minor inconsistencies in formatting. While handling the large XML file is very inefficient and slow, once the data in transferred to the SQL database, the data can be querried in a really fast way trough the command line interface or the Python API.\n",
    "\n",
    "During data analysis I looked at a few interesting features such as the different users that contributed to the data set and the different restaurants and sport centers available in Zürich. I also touched on the `mplleaflet` library for the display of geographical nodes on a interactive `JavaScript` map. Additionally, it could also be used for the visualisation of polygenic ways and relations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
